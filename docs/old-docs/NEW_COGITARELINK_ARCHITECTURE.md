# New CogitareLink Architecture: Living Scientific Assistant

**Vision**: A semantic web-powered scientific research assistant that combines rigorous knowledge management with intelligent discovery and continuous learning from agent interactions.

## Core Philosophy

- **Hybrid CLI + Agentic**: CLI-first composable tools with rich structured responses for agent intelligence
- **Semantic Memory**: All discovered knowledge stored as immutable entities with full provenance tracking
- **Discovery-First Guardrails**: Never query without schema understanding (like Read-before-Edit in Claude Code)  
- **In-Context Teaching**: Continuously learn and improve from actual agent usage patterns
- **Software 2.0**: Generalized tools + intelligent prompting rather than hardcoded logic
- **Verifiable Science**: Every conclusion traceable to sources with cryptographic verification
- **Framework Agnostic**: Works with Claude Code, DSPy, LangGraph, or any agent framework

## Hybrid Architecture: CLI + Agentic Intelligence

**Design Principle**: CLI-first composable tools with rich structured responses that provide sophisticated agent guidance.

```
cogitarelink/
â”œâ”€â”€ core/                           # Semantic memory micro-kernel (~1500 LOC)
â”‚   â”œâ”€â”€ entity.py                  # Immutable JSON-LD entities with SHA-256 signatures
â”‚   â”œâ”€â”€ graph.py                   # Multi-backend RDF storage (RDFLib/memory) with named graphs
â”‚   â”œâ”€â”€ context.py                 # Full JSON-LD 1.1 processor (type-scoped, @nest, @propagate)
â”‚   â”œâ”€â”€ cache.py                   # Multi-layer caching (memory LRU + disk + TTL)
â”‚   â”œâ”€â”€ processor.py               # Entity processing pipeline with normalization
â”‚   â””â”€â”€ temporal.py                # OWL-Time temporal reasoning and event modeling
â”‚
â”œâ”€â”€ vocab/                          # Advanced vocabulary management (from cogitarelink)
â”‚   â”œâ”€â”€ registry.py                # Dynamic vocab registry with versioning and aliases
â”‚   â”œâ”€â”€ composer.py                # Context composition with JSON-LD 1.1 features
â”‚   â”œâ”€â”€ collision.py               # Multi-strategy conflict resolution (property_scoped, etc.)
â”‚   â””â”€â”€ alignment.py               # Cross-vocabulary property alignment with confidence
â”‚
â”œâ”€â”€ reason/                         # SHACL-unified reasoning engine
â”‚   â”œâ”€â”€ shacl_engine.py           # Universal SHACL SPARQL rules (replaces OWL-RL)
â”‚   â”œâ”€â”€ prov.py                    # W3C PROV-O provenance wrapper for all operations
â”‚   â”œâ”€â”€ obqc.py                    # Ontology-based query checking and validation
â”‚   â”œâ”€â”€ temporal_reasoning.py      # Temporal relation inference (before, after, contains)
â”‚   â”œâ”€â”€ materialization.py        # Auto-materialize triples from discovered ontologies
â”‚   â””â”€â”€ explanation.py             # Generate complete reasoning chain explanations
â”‚
â”œâ”€â”€ intelligence/                   # Agent intelligence layer (from wikidata-mcp)
â”‚   â”œâ”€â”€ discovery_engine.py       # Multi-method endpoint discovery with fallbacks
â”‚   â”œâ”€â”€ complexity_analyzer.py    # Query complexity analysis and risk scoring
â”‚   â”œâ”€â”€ guidance_generator.py     # Domain-specific reasoning scaffolds and CoT patterns
â”‚   â”œâ”€â”€ guardrails.py             # Discovery-first validation system
â”‚   â”œâ”€â”€ universal_resolver.py     # Self-configuring identifier resolution
â”‚   â””â”€â”€ workflow_orchestrator.py  # Multi-step research workflow intelligence
â”‚
â”œâ”€â”€ memory/                         # Semantic memory with introspection
â”‚   â”œâ”€â”€ semantic_cache.py          # Queryable semantic memory with SPARQL interface
â”‚   â”œâ”€â”€ introspection.py          # Tools for agents to query their own knowledge
â”‚   â”œâ”€â”€ materialization_cache.py  # Cache materialized triples with provenance
â”‚   â””â”€â”€ state_manager.py          # Discovery state tracking (like Claude Code file tracking)
â”‚
â”œâ”€â”€ verify/                         # Trust and verification layer
â”‚   â”œâ”€â”€ validator.py               # SHACL validation with suggestion generation
â”‚   â”œâ”€â”€ signer.py                  # Ed25519 signing with URDNA2015 canonicalization
â”‚   â”œâ”€â”€ consistency.py             # Logical consistency checking
â”‚   â””â”€â”€ quality_scoring.py         # Data quality and confidence metrics
â”‚
â”œâ”€â”€ teaching/                       # In-Context Teaching system
â”‚   â”œâ”€â”€ interaction_logger.py      # Log all agent interactions with context
â”‚   â”œâ”€â”€ pattern_analyzer.py        # Analyze usage patterns and success factors
â”‚   â”œâ”€â”€ prompt_optimizer.py        # Auto-optimize prompts based on actual usage
â”‚   â”œâ”€â”€ heuristic_learner.py       # Learn domain-specific heuristics from successes
â”‚   â”œâ”€â”€ feedback_collector.py      # Collect explicit and implicit feedback
â”‚   â””â”€â”€ live_updater.py            # Hot-reload tool configurations and prompts
â”‚
â”œâ”€â”€ cli/                            # CLI tools with structured intelligence
â”‚   â”œâ”€â”€ discover.py                # cl_discover - Scientific resource discovery
â”‚   â”œâ”€â”€ sparql.py                  # cl_sparql - Guardrail-protected SPARQL queries
â”‚   â”œâ”€â”€ materialize.py             # cl_materialize - Knowledge materialization
â”‚   â”œâ”€â”€ explain.py                 # cl_explain - Reasoning chain explanation
â”‚   â”œâ”€â”€ validate.py                # cl_validate - SHACL validation with suggestions
â”‚   â”œâ”€â”€ query_memory.py            # cl_query_memory - Semantic memory queries
â”‚   â”œâ”€â”€ resolve.py                 # cl_resolve - Universal identifier resolution
â”‚   â””â”€â”€ orchestrate.py             # cl_orchestrate - Multi-step workflow coordination
â”‚
â”œâ”€â”€ adapters/                       # Framework-agnostic integration
â”‚   â”œâ”€â”€ claude_code.py             # Claude Code tool registration and proxying
â”‚   â”œâ”€â”€ openai_functions.py        # OpenAI function calling integration
â”‚   â”œâ”€â”€ dspy_tools.py             # DSPy tool integration
â”‚   â””â”€â”€ universal_cli.py           # Direct CLI access for any framework
â”‚
â”œâ”€â”€ testing/                        # Development and testing infrastructure
â”‚   â”œâ”€â”€ iterative_testing.py      # Progressive testing with immediate feedback
â”‚   â”œâ”€â”€ integration_tests.py      # Tool composition and workflow testing
â”‚   â”œâ”€â”€ cli_testing.py            # CLI golden-file tests
â”‚   â”œâ”€â”€ teaching_simulation.py    # Simulate agent learning scenarios
â”‚   â””â”€â”€ provenance_testing.py     # Test provenance tracking integrity
â”‚
â””â”€â”€ config/                         # Live configuration system
    â”œâ”€â”€ prompts/                    # Editable prompt templates with learning metadata
    â”œâ”€â”€ heuristics/                 # Learned heuristics database with confidence scores
    â”œâ”€â”€ patterns/                   # Successful interaction patterns
    â”œâ”€â”€ ontologies/                 # Domain ontology configurations
    â””â”€â”€ endpoints/                  # Discovered endpoint schemas and capabilities
```

## CLI Tools with Structured Intelligence

### Design Pattern: Rich JSON Responses for Agent Guidance

Each CLI tool returns structured JSON responses that provide sophisticated agent guidance while maintaining CLI composability:

**Response Structure (from wikidata-mcp)**
```json
{
    "success": true,
    "data": {...},
    "metadata": {
        "execution_time_ms": 1250,
        "materialized_triples": 156,
        "complexity_score": 3.2,
        "cached": false
    },
    "suggestions": {
        "next_tools": ["cl_sparql --endpoint discovered --query '...'"],
        "reasoning_patterns": ["ðŸ§¬ PROTEIN â†’ PATHWAY â†’ DISEASE reasoning chain"],
        "workflow_steps": ["1. Extract cross-references", "2. Follow to databases"]
    },
    "claude_guidance": {
        "domain_context": ["Biological research patterns discovered"],
        "optimization_hints": ["Query complexity acceptable for this endpoint"],
        "potential_issues": ["Check for missing temporal coverage"]
    }
}
```

### 1. CLI Discovery Tool with Semantic Materialization

**cl_discover** - Combines wikidata-mcp intelligence with cogitarelink semantic memory:

```python
# cli/discover.py
@click.command()
@click.argument('query')
@click.option('--domains', multiple=True, help='Research domains: biology, climate, etc.')
@click.option('--materialize/--no-materialize', default=True, help='Auto-materialize semantic context')
@click.option('--teaching-mode/--no-teaching', default=True, help='Enable learning from interaction')
@click.option('--format', default='jsonld', help='Output format: jsonld, ttl, ndjson')
def discover(query, domains, materialize, teaching_mode, format):
    """Discover scientific resources with auto-materialization and agent guidance"""
    
    # Core discovery with multi-method fallbacks (from wikidata-mcp)
    result = await discovery_engine.discover_scientific_resources(
        query, domains, use_fallbacks=True
    )
    
    materialized_count = 0
    if materialize:
        # Auto-materialize semantic context (from cogitarelink)
        for resource in result["resources"]:
            # Identify relevant ontologies
            ontologies = await _identify_domain_ontologies(resource, domains)
            
            # Create immutable entity with SHA-256 signature
            entity = Entity(
                vocab=resource["vocabularies"],
                content=resource["metadata"],
                meta={"source": resource["url"], "discovered_at": datetime.utcnow()}
            )
            
            # Apply SHACL rules to materialize implicit knowledge
            with wrap_patch_with_prov(graph, agent="cli_discover", activity="discovery_materialization"):
                materialized_triples = await shacl_engine.materialize_from_entity(entity, ontologies)
                entity = entity.with_materialized(materialized_triples)
                materialized_count += len(materialized_triples)
            
            # Store in semantic cache with provenance
            await semantic_cache.store_entity(entity, track_provenance=True)
    
    # Generate rich agent guidance (from wikidata-mcp patterns)
    guidance = await guidance_generator.generate_discovery_guidance(
        result["resources"], domains, materialized_count
    )
    
    # Log interaction for teaching
    if teaching_mode:
        await teaching_system.log_interaction(
            tool="cl_discover",
            inputs={"query": query, "domains": list(domains)},
            outputs=result,
            success=True
        )
    
    # Structured response for agents
    response = {
        "success": True,
        "data": {
            "resources": result["resources"],
            "materialized_triples": materialized_count,
            "endpoints_discovered": len([r for r in result["resources"] if r.get("sparql_endpoint")])
        },
        "metadata": {
            "execution_time_ms": result["timing"],
            "discovery_methods_used": result["methods"],
            "domains_analyzed": list(domains),
            "semantic_memory_updated": materialized_count > 0
        },
        "suggestions": {
            "next_tools": guidance["next_tools"],
            "reasoning_patterns": guidance["reasoning_patterns"],
            "workflow_steps": guidance["workflow_steps"]
        },
        "claude_guidance": {
            "discovered_capabilities": guidance["endpoint_capabilities"],
            "domain_intelligence": guidance["domain_patterns"],
            "materialization_status": f"Added {materialized_count} triples to semantic memory"
        }
    }
    
    click.echo(json.dumps(response, indent=2))
```

### 2. CLI SPARQL Tool with Discovery Guardrails

**cl_sparql** - Combines wikidata-mcp guardrails with cogitarelink materialization:

```python
# cli/sparql.py
@click.command()
@click.argument('query')
@click.option('--endpoint', default='wikidata', help='SPARQL endpoint or alias')
@click.option('--materialize/--no-materialize', default=True, help='Auto-materialize results')
@click.option('--explain-complexity/--no-complexity', default=True, help='Analyze query complexity')
@click.option('--format', default='jsonld', help='Output format')
def sparql(query, endpoint, materialize, explain_complexity, format):
    """SPARQL query with discovery guardrails and semantic materialization"""
    
    # Discovery-first guardrail (from wikidata-mcp)
    if not await state_manager.is_discovered(endpoint):
        error_response = {
            "success": False,
            "error": {
                "code": "DISCOVERY_REQUIRED",
                "message": f"Endpoint '{endpoint}' must be discovered before querying",
                "required_action": f"cl_discover --endpoint {endpoint}",
                "reasoning": "Schema discovery provides vocabulary context and prevents errors",
                "suggestions": [
                    f"Run: cl_discover '{endpoint} datasets'",
                    "Discovery follows the same pattern as Read-before-Edit in Claude Code",
                    "This prevents common vocabulary and syntax errors"
                ]
            }
        }
        click.echo(json.dumps(error_response, indent=2))
        return
    
    # Query complexity analysis (from wikidata-mcp)
    if explain_complexity:
        complexity = await complexity_analyzer.analyze_query(query, endpoint)
        if complexity["risk_level"] == "very_high":
            error_response = {
                "success": False,
                "error": {
                    "code": "QUERY_TOO_COMPLEX",
                    "risk_score": complexity["score"],
                    "performance_prediction": complexity["prediction"],
                    "optimization_suggestions": complexity["suggestions"]
                }
            }
            click.echo(json.dumps(error_response, indent=2))
            return
    
    # Execute with provenance tracking (from cogitarelink)
    with wrap_patch_with_prov(graph, agent="cli_sparql", activity="sparql_query"):
        result = await sparql_engine.execute_with_provenance(query, endpoint)
    
    # Auto-materialize results into semantic memory
    materialized_count = 0
    if materialize and result["success"]:
        materialized_entities = await _materialize_sparql_results(
            result["data"], 
            endpoint, 
            query_provenance=f"sparql:{hash(query)}"
        )
        materialized_count = len(materialized_entities)
        
        # Store materialized entities
        for entity in materialized_entities:
            await semantic_cache.store_entity(entity, track_provenance=True)
    
    # Enhanced response with guidance
    response = {
        "success": True,
        "data": result["data"],
        "metadata": {
            "execution_time_ms": result["execution_time"],
            "complexity_score": complexity["score"] if explain_complexity else None,
            "risk_level": complexity["risk_level"] if explain_complexity else None,
            "materialized_triples": materialized_count,
            "endpoint": endpoint
        },
        "suggestions": {
            "next_tools": [
                f"cl_explain --triple 'result_triple_hash'",
                f"cl_query_memory --query 'related queries'"
            ],
            "optimization_hints": complexity.get("optimization_hints", []),
            "related_queries": result.get("suggested_queries", [])
        },
        "claude_guidance": {
            "result_interpretation": result.get("result_patterns", []),
            "follow_up_opportunities": result.get("follow_up_suggestions", []),
            "materialization_status": f"Added {materialized_count} entities to semantic memory" if materialized_count > 0 else "No materialization performed"
        }
    }
    
    click.echo(json.dumps(response, indent=2))
```

### 3. Framework-Agnostic Adapter Layer

**Claude Code Integration** - Proxies CLI tools as Claude Code functions:

```python
# adapters/claude_code.py
from typing import List, Dict, Any
import subprocess
import json

@claude_tool
async def discover_science(
    query: str, 
    domains: List[str] = None,
    materialize: bool = True
) -> Dict[str, Any]:
    """Discover scientific resources with auto-materialization and agent guidance"""
    
    # Build CLI command
    cmd = ['cl_discover', query]
    if domains:
        cmd.extend(['--domains'] + domains)
    if not materialize:
        cmd.append('--no-materialize')
    cmd.extend(['--format', 'jsonld'])
    
    # Execute CLI tool
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    
    if result.returncode != 0:
        return {
            "success": False,
            "error": {
                "code": "CLI_ERROR",
                "message": result.stderr,
                "command": ' '.join(cmd)
            }
        }
    
    # Parse structured JSON response
    return json.loads(result.stdout)

@claude_tool
async def sparql_query(
    query: str,
    endpoint: str = "wikidata", 
    materialize: bool = True
) -> Dict[str, Any]:
    """SPARQL query with discovery guardrails and complexity analysis"""
    
    cmd = ['cl_sparql', query, '--endpoint', endpoint]
    if not materialize:
        cmd.append('--no-materialize')
    cmd.extend(['--format', 'jsonld'])
    
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    
    if result.returncode != 0:
        return json.loads(result.stdout)  # CLI returns structured errors
    
    return json.loads(result.stdout)

@claude_tool
async def query_memory(
    query: str,
    explain: bool = True
) -> Dict[str, Any]:
    """Query semantic memory with OBQC validation and reasoning"""
    
    cmd = ['cl_query_memory', query]
    if explain:
        cmd.append('--explain')
    
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    return json.loads(result.stdout)

@claude_tool  
async def explain_reasoning(
    conclusion: str,
    include_provenance: bool = True
) -> Dict[str, Any]:
    """Explain reasoning chains with full provenance and temporal context"""
    
    cmd = ['cl_explain', conclusion]
    if include_provenance:
        cmd.append('--include-provenance')
        
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    return json.loads(result.stdout)

# Auto-register all CLI tools as Claude Code functions
def enable_claude_tools():
    """Enable all CogitareLink tools for Claude Code"""
    # Tools are automatically available once this module is imported
    pass
```

### 4. In-Context Teaching System Integration

**Teaching-Enhanced CLI Tools** - All CLI tools automatically log interactions for learning:

```python
# teaching/interaction_logger.py
class CLITeachingLogger:
    """Log CLI tool interactions for continuous learning"""
    
    def log_cli_interaction(self, tool_name: str, args: Dict, result: Dict, 
                           success: bool, execution_time: float):
        """Log CLI tool interaction with rich context"""
        
        interaction = {
            "timestamp": datetime.utcnow().isoformat(),
            "tool": tool_name,
            "interface": "CLI",
            "inputs": args,
            "outputs": result,
            "success": success,
            "execution_time_ms": execution_time * 1000,
            "context": {
                "session_id": self._get_session_id(),
                "research_domain": self._infer_domain(args),
                "agent_framework": self._detect_framework()  # Claude Code, direct CLI, etc.
            }
        }
        
        # Immediate pattern analysis
        asyncio.create_task(self.analyze_interaction(interaction))
    
    async def analyze_interaction(self, interaction: Dict):
        """Real-time analysis for learning"""
        
        if interaction["tool"] == "cl_discover" and interaction["success"]:
            # Learn successful discovery patterns
            await self._learn_discovery_patterns(interaction)
        
        elif interaction["tool"] == "cl_sparql":
            if interaction["success"]:
                # Learn effective query patterns
                await self._learn_sparql_patterns(interaction)
            else:
                # Learn from failures to improve guardrails
                await self._learn_failure_patterns(interaction)

# CLI wrapper with teaching integration
def with_teaching(cli_func):
    """Decorator to add teaching to CLI commands"""
    
    @wraps(cli_func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            # Execute CLI command
            result = cli_func(*args, **kwargs)
            success = True
        except Exception as e:
            result = {"success": False, "error": str(e)}
            success = False
        
        # Log for teaching
        teaching_logger.log_cli_interaction(
            tool_name=cli_func.__name__,
            args=kwargs,
            result=result,
            success=success,
            execution_time=time.time() - start_time
        )
        
        return result
    
    return wrapper

# Enhanced CLI commands with teaching
@click.command()
@with_teaching
def discover(query, domains, materialize, teaching_mode, format):
    """Discovery with automatic teaching integration"""
    # Implementation from above
    pass
```

### 5. Complete CLI Tool Set

**8 Core CLI Tools** with rich structured responses:

| Tool | Purpose | Key Features |
|------|---------|--------------|
| `cl_discover` | Scientific resource discovery | Multi-method fallbacks, auto-materialization, domain intelligence |
| `cl_sparql` | SPARQL queries with guardrails | Complexity analysis, discovery-first validation, provenance tracking |
| `cl_materialize` | Explicit knowledge materialization | SHACL SPARQL rules, convergence detection, delta tracking |
| `cl_explain` | Reasoning chain explanation | PROV-O traces, temporal context, confidence scoring |
| `cl_validate` | SHACL validation with suggestions | Consistency checking, improvement suggestions, quality scoring |
| `cl_query_memory` | Semantic memory queries | OBQC validation, introspection, cached results |
| `cl_resolve` | Universal identifier resolution | Self-configuring, cross-reference navigation, confidence scoring |
| `cl_orchestrate` | Multi-step workflow coordination | Research templates, checkpoint/resume, parallel execution |

### 6. Example Agent Workflows with Structured Intelligence

**Complete Research Workflow Example** - Combining wikidata-mcp intelligence with cogitarelink semantic memory:

```bash
# Scientific discovery with materialization
cl_discover "SARS-CoV-2 spike protein" --domains biology virology --materialize
# Returns structured JSON with next_tools suggestions

# Follow suggested SPARQL query with guardrails
cl_sparql "SELECT ?protein ?structure WHERE { ?protein wdt:P31 wd:Q8054 }" --endpoint wikidata
# Discovery-first validation prevents schema errors

# Query materialized semantic memory
cl_query_memory "What spike protein structures have been discovered?"
# OBQC validation ensures query correctness

# Explain reasoning chain with provenance
cl_explain "SARS-CoV-2 spike protein binds ACE2 receptor"
# Full PROV-O trace with temporal context
```

### 7. Development and Testing Infrastructure

**Iterative Development Pattern** - Combining both CLI testing and workflow validation:

```python
# CLI golden-file tests
def test_cli_tools():
    """Test CLI tools with known good outputs"""
    
    # Test discovery tool
    result = subprocess.run(['cl_discover', 'insulin protein', '--domains', 'biology'], 
                          capture_output=True, text=True)
    expected = load_golden_file('discover_insulin.json')
    assert json.loads(result.stdout) == expected
    
    # Test SPARQL with guardrails
    result = subprocess.run(['cl_sparql', 'SELECT ?protein WHERE { ?protein wdt:P31 wd:Q8054 }', 
                          '--endpoint', 'wikidata'], capture_output=True, text=True)
    assert result.returncode == 11  # Discovery required error
    
# Integration workflow tests  
async def test_complete_scientific_workflow():
    """Test full scientific workflow via Claude Code tools"""
    
    print("ðŸ§ª Testing complete scientific workflow...")
    
    # 1. Discovery with materialization (via Claude Code tool)
    discovery = await discover_science(
        "SARS-CoV-2 spike protein", 
        domains=["biology", "virology"],
        materialize=True
    )
    assert discovery["success"] == True
    assert discovery["data"]["materialized_triples"] > 0
    
    # 2. Query semantic memory  
    memory_query = await query_memory(
        "What spike protein structures have been materialized?"
    )
    assert len(memory_query["data"]["results"]) > 0
    
    # 3. SPARQL with guardrails
    sparql_result = await sparql_query(
        "SELECT ?protein ?structure WHERE { ?protein wdt:P31 wd:Q8054 }",
        endpoint="wikidata",
        materialize=True
    )
    assert sparql_result["success"] == True
    
    # 4. Explain reasoning chain
    explanation = await explain_reasoning(
        "SARS-CoV-2 spike protein binds ACE2 receptor",
        include_provenance=True
    )
    assert "prov:Activity" in explanation["data"]["reasoning_chain"]
    
    print("ðŸŽ‰ Complete workflow test passed!")
    
    # Teaching: Log successful workflow
    await teaching_system.log_workflow_success([
        discovery, memory_query, sparql_result, explanation
    ])
```

## Installation and Usage

```bash
# Install the complete system
pip install cogitarelink

# Enable Claude Code integration
python -c "
from cogitarelink.adapters.claude_code import enable_claude_tools
enable_claude_tools(teaching_mode=True)
"

# Claude Code now has access to:
# - Scientific resource discovery with semantic materialization
# - SPARQL queries with discovery guardrails and complexity analysis  
# - Semantic memory with provenance tracking and reasoning
# - Universal identifier resolution with cross-reference navigation
# - Complete reasoning explanation with temporal context
# - Continuous learning from usage patterns
```

**CLI Usage** (Direct command-line access):
```bash
# Scientific discovery with auto-materialization
cl_discover "SARS-CoV-2 spike protein" --domains biology virology --materialize

# SPARQL with discovery guardrails  
cl_sparql "SELECT ?protein WHERE { ?protein wdt:P31 wd:Q8054 }" --endpoint wikidata

# Query semantic memory
cl_query_memory "What spike proteins have been discovered?"

# Explain reasoning with provenance
cl_explain "SARS-CoV-2 spike protein binds ACE2 receptor" --include-provenance
```

## Key Innovations

### 1. **Hybrid CLI + Agentic Architecture**
- **CLI-first composable tools** with rich structured responses for agent intelligence
- **Framework-agnostic design** supporting Claude Code, DSPy, LangGraph, and direct CLI
- **Universal adapter layer** enabling seamless integration across agent platforms

### 2. **Discovery-First Guardrails**  
- **Schema discovery required** before SPARQL queries (like Read-before-Edit in Claude Code)
- **Complexity analysis** with performance prediction and risk scoring
- **Automatic endpoint capabilities** detection with fallback strategies

### 3. **Semantic Memory with Provenance**
- **Immutable entities** with SHA-256 signatures and cryptographic verification
- **Full provenance tracking** using W3C PROV-O for every operation and reasoning step
- **Multi-layer semantic caching** with queryable SPARQL interface for introspection

### 4. **SHACL-Unified Reasoning Engine**
- **Single reasoning engine** using SHACL SPARQL rules (replaces separate OWL-RL + SHACL)
- **Auto-materialization** of implicit knowledge from discovered ontologies
- **Remote materialization** across federated SPARQL endpoints with provenance

### 5. **In-Context Teaching System**
- **Continuous learning** from actual Claude Code usage patterns and success/failure modes
- **Real-time prompt optimization** based on agent interaction effectiveness
- **Heuristic learning** with domain-specific pattern recognition and confidence scoring

### 6. **Advanced JSON-LD 1.1 Processing**
- **Complete JSON-LD 1.1 support** including type-scoped contexts, @nest, @propagate
- **Context composition** with collision detection and multi-strategy resolution
- **Vocabulary alignment** with confidence scoring and cross-vocabulary property mapping

### 7. **Agent Intelligence Layer**
- **Structured responses** with claude_guidance sections for sophisticated agent reasoning
- **Workflow orchestration** with research templates and checkpoint/resume capabilities  
- **Universal identifier resolution** with cross-reference navigation and confidence scoring

## Architecture Synthesis

This architecture creates a **living, learning scientific assistant** that successfully combines:

- **Original cogitarelink's semantic rigor**: Immutable entities, SHACL reasoning, provenance tracking, verification
- **wikidata-mcp's exceptional agent intelligence**: Structured responses, discovery-first guardrails, complexity analysis, Chain-of-Thought scaffolds
- **Claude Code compatibility**: CLI-first tools with rich JSON responses, framework-agnostic adapters
- **Software 2.0 approach**: Generalized tools + intelligent prompting rather than hardcoded logic

**Result**: A unified system that provides both CLI composability AND sophisticated agent guidance, continuously improving through real-world usage with Claude Code while maintaining the semantic web's scientific rigor.